mode: train_sft
data:
  name: "Salesforce/wikitext"
  path: ./data/wikitext-2-raw-v1
  save_path: ./data/processed_data
  train_batch_size: 8
  micro_batch_size_per_gpu: 2
  train_name: train
  val_name: eval
  prompt_key: None
  response_key: None
  max_length: 1024
  balance_dp_token: True
  err_prob: 0.05
  noise_prob_policy: Linear/Fixed/Importance
model:
  path_from_pretrained: ./
  fsdp_config:
    pass: pass
  check_point_saves: ./
  lora:
    open: False
    alpha: 16
    rank: 8
    target_modules: head
optim:
  lr: 5e-5
  betas: [0.9,0.95]
  weight_decay: 0.01
  warmup_steps_ratio: 0.1
  clip_grad: 1.0
trainer:
  default_local_dir: ./sft_model
  default_hdfs_dir: hdfs://tmp/experiments/gsm8k/gemma-1.1-7b-it/
  project_name: shushurun-sft
  experiment_name: test
  total_epochs: 3
  total_training_steps: null
  logger: ['console']
  save_checkpoint_steps: 10
  early_stopping:
    open: True
    monitor: eval_loss
    patience: 3
    delta: 0.01
diffusion:
  diffusion_steps: 64
  decoding_strategy: stochastic0.5-linear
  token_reweighting: true
  alpha: 0.25
  gamma: 2
